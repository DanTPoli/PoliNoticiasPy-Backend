import re
import time
from datetime import datetime
from urllib.parse import urljoin

try:
    from playwright.sync_api import sync_playwright
    from bs4 import BeautifulSoup
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False

from scraper.content_extractor import extrair_primeiro_paragrafo

# Padr√£o de URL de not√≠cia: /2026/01/23/titulo-da-materia/
REGEX_DATA_NEWS = re.compile(r'/\d{4}/\d{2}/\d{2}/')

def coletar_brasil_de_fato():
    if not PLAYWRIGHT_AVAILABLE:
        print("‚ö†Ô∏è [BdF] Playwright n√£o dispon√≠vel.")
        return []

    BASE_URL = "https://www.brasildefato.com.br/"
    noticias_coletadas = []
    urls_vistas = set()

    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            context = browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            )
            page = context.new_page()
            
            print(f"üöÄ [BdF] Acessando Home...")
            page.goto(BASE_URL, wait_until="domcontentloaded", timeout=60000)
            
            # Espera o Elementor carregar os blocos de not√≠cia
            page.wait_for_selector(".e-loop-item", timeout=20000)
            
            soup = BeautifulSoup(page.content(), 'html.parser')
            browser.close()

            # Estrat√©gia: Buscamos as manchetes dentro dos Heading Titles do Elementor
            manchetes_tags = soup.find_all(['h2', 'h3'], class_='elementor-heading-title')

            for tag in manchetes_tags:
                link_tag = tag.find('a', href=REGEX_DATA_NEWS)
                if not link_tag:
                    continue

                url_completa = urljoin(BASE_URL, link_tag['href'])
                titulo_candidato = link_tag.get_text().strip()

                # --- FILTROS INTELIGENTES ---
                
                # 1. Ignora "Chap√©us" (Tags curtas como 'TENS√ÉO', '√â GREVE', 'RUMO AO OSCAR')
                # Manchetes reais do BdF dificilmente t√™m menos de 35 caracteres.
                if len(titulo_candidato) < 35:
                    continue

                # 2. Evita duplicados (A home repete links em diferentes blocos)
                if url_completa in urls_vistas:
                    continue

                if len(noticias_coletadas) >= 10: 
                    break

                print(f"   [BdF] Lendo lead real: {titulo_candidato[:50]}...")
                
                # DEEP SCRAPING: Acessa a p√°gina da not√≠cia para pegar o lead real
                conteudo_lead = extrair_primeiro_paragrafo(url_completa)

                # Limpeza de boilerplate que possa ter vindo no lead
                if conteudo_lead:
                    # Se o lead vier com o aviso de copyright, cortamos ele
                    if "Todos os conte√∫dos" in conteudo_lead:
                        conteudo_lead = conteudo_lead.split("Todos os conte√∫dos")[0].strip()
                    
                    texto_ia = f"{titulo_candidato}. {conteudo_lead}"
                else:
                    texto_ia = titulo_candidato

                noticias_coletadas.append({
                    "nome_fonte": "Brasil de Fato",
                    "titulo": titulo_candidato,
                    "url": url_completa,
                    "texto_analise_ia": texto_ia,
                    "vi√©s_classificado": None,
                    "id_cluster": None,
                    "data_coleta": datetime.now().isoformat()
                })
                
                urls_vistas.add(url_completa)

            print(f"‚úÖ Brasil de Fato: {len(noticias_coletadas)} not√≠cias filtradas com sucesso.")
            return noticias_coletadas

    except Exception as e:
        print(f"‚ùå Erro Cr√≠tico no Scraper BdF: {e}")
        return []